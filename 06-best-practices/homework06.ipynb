{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59cb0645",
   "metadata": {},
   "source": [
    "# Homework Best Prctices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0936962",
   "metadata": {},
   "source": [
    "## Q1. Refactoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db1a190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 06:30:31 - homework06.batch_refactoring - INFO - Starting prediction for year=2023, month=3\n",
      "2025-07-07 06:30:31 - homework06.batch_refactoring - INFO - Input file: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "2025-07-07 06:30:31 - homework06.batch_refactoring - INFO - Output file: homework06/data/taxi_type=yellow_year=2023_month=03.parquet\n",
      "2025-07-07 06:30:31 - homework06.batch_refactoring - INFO - Loading model...\n",
      "2025-07-07 06:30:32 - homework06.batch_refactoring - INFO - Model loaded successfully from /workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/model.bin\n",
      "2025-07-07 06:30:32 - homework06.batch_refactoring - INFO - Reading data from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "2025-07-07 06:30:35 - homework06.batch_refactoring - INFO - Read 3403766 records from https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "2025-07-07 06:30:39 - homework06.batch_refactoring - INFO - After preparation: 3316216 records remaining\n",
      "2025-07-07 06:30:40 - homework06.batch_refactoring - INFO - Transforming features...\n",
      "2025-07-07 06:30:50 - homework06.batch_refactoring - INFO - Making predictions...\n",
      "2025-07-07 06:30:50 - homework06.batch_refactoring - INFO - \n",
      "Predicted mean duration: 14.20\n",
      "2025-07-07 06:30:50 - homework06.batch_refactoring - INFO - \n",
      "Predicted sum duration: 47103086.51\n",
      "\n",
      "2025-07-07 06:30:50 - homework06.batch_refactoring - INFO - Saving results to homework06/data/taxi_type=yellow_year=2023_month=03.parquet\n",
      "2025-07-07 06:30:51 - homework06.batch_refactoring - INFO - Results saved successfully to homework06/data/taxi_type=yellow_year=2023_month=03.parquet\n",
      "2025-07-07 06:30:51 - homework06.batch_refactoring - INFO - Total predictions: 3316216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output saved to: homework06/data/taxi_type=yellow_year=2023_month=03.parquet\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import os\n",
    "from sklearn.exceptions import InconsistentVersionWarning\n",
    "from homework06.batch_refactoring import main\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=InconsistentVersionWarning)\n",
    "\n",
    "# Use a local path for output to test Q1 (no S3 needed)\n",
    "# Temporarily unset any environment variables that might affect the path\n",
    "if \"INPUT_FILE_PATTERN\" in os.environ:\n",
    "    del os.environ[\"INPUT_FILE_PATTERN\"]\n",
    "if \"OUTPUT_FILE_PATTERN\" in os.environ:\n",
    "    del os.environ[\"OUTPUT_FILE_PATTERN\"]\n",
    "if \"S3_ENDPOINT_URL\" in os.environ:\n",
    "    del os.environ[\"S3_ENDPOINT_URL\"]\n",
    "\n",
    "# Now run the main function which will use the default paths\n",
    "output_path = main(2023, 3)\n",
    "print(f\"Output saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c667e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ANSWER TO Q1: The correct if statement for the 'main' block is:\n",
      "if __name__ == \"__main__\":\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ANSWER TO Q1: The correct if statement for the 'main' block is:\")\n",
    "print('if __name__ == \"__main__\":')\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1bf73",
   "metadata": {},
   "source": [
    "## Q2. Installing pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fd9fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1;32mInstalling pytest...\u001b[0m\n",
      "✔ Installation Succeeded\n",
      "To activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(b54bd8)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[1;32mUpgrading\u001b[0m pytest in  dependencies.\n",
      "\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies...es...\n",
      "\u001b[2K✔ Success! Locking dev-packages...\n",
      "\u001b[2K\u001b[32m⠹\u001b[0m Locking dev-packages...\n",
      "\u001b[1A\u001b[2K\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies...es...\n",
      "\u001b[2K✔ Success! Locking dev-packages...\n",
      "\u001b[2K\u001b[32m⠸\u001b[0m Locking dev-packages...\n",
      "\u001b[1A\u001b[2K\u001b[?25lBuilding requirements...\n",
      "\u001b[2KResolving dependencies...es...\n",
      "\u001b[2K✔ Success! Locking dev-packages...\n",
      "\u001b[2K\u001b[32m⠙\u001b[0m Locking dev-packages...\n",
      "\u001b[1A\u001b[2KTo activate this project's virtualenv, run \u001b[33mpipenv shell\u001b[0m.\n",
      "Alternatively, run a command inside the virtualenv with \u001b[33mpipenv run\u001b[0m.\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(b54bd8)...\u001b[0m\n",
      "\u001b[32mAll dependencies are now up-to-date!\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1;39m(b54bd8)...\u001b[0m\n",
      "\u001b[1mInstalling dependencies from Pipfile.lock \u001b[0m\u001b[1m(b54bd8)...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pipenv install --dev pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d865a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 48\n",
      "-rw-rw-rw-  1 codespace codespace   87 Jul  7 06:32 __init__.py\n",
      "drwxrwxrwx+ 2 codespace codespace 4096 Jul  4 17:33 __pycache__\n",
      "-rw-rw-rw-  1 codespace codespace  803 Jul  4 16:38 fix_imports.py\n",
      "-rw-rw-rw-  1 codespace codespace 6874 Jul  4 18:01 integration_test.py\n",
      "-rw-rw-rw-  1 codespace codespace 3764 Jul  4 16:38 run_integration_tests.sh\n",
      "-rw-rw-rw-  1 codespace codespace 4531 Jul  4 16:38 run_tests.sh\n",
      "-rw-rw-rw-  1 codespace codespace 4004 Jul  4 16:38 setup_s3.py\n",
      "-rw-rw-rw-  1 codespace codespace 5131 Jul  7 06:32 test_batch_refactoring.py\n",
      "-rw-rw-rw-  1 codespace codespace 2078 Jul  4 18:00 test_integration.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.makedirs(\"homework06/tests\", exist_ok=True)\n",
    "\n",
    "# create the files inside the tests folder\n",
    "!touch homework06/tests/test_batch_refactoring.py\n",
    "!touch homework06/tests/__init__.py\n",
    "\n",
    "!ls -l homework06/tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08eec600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "ANSWER TO Q2: The other file needed in the tests directory is __init__.py\n",
      "This file allows Python to recognize the tests directory as a package, enabling imports.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"ANSWER TO Q2: The other file needed in the tests directory is __init__.py\")\n",
    "print(\n",
    "    \"This file allows Python to recognize the tests directory as a package, enabling imports.\"\n",
    ")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c431dd6",
   "metadata": {},
   "source": [
    "## Q3. Writing first unit test\n",
    "\n",
    "How many rows should be there in the expected dataframe?\n",
    "\n",
    "* 1\n",
    "* 2 ✅\n",
    "* 3\n",
    "* 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd335737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytest in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (8.4.1)\n",
      "Requirement already satisfied: exceptiongroup>=1 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from pytest) (1.3.0)\n",
      "Requirement already satisfied: iniconfig>=1 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from pytest) (2.1.0)\n",
      "Requirement already satisfied: packaging>=20 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from pytest) (25.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from pytest) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from pytest) (2.19.2)\n",
      "Requirement already satisfied: tomli>=1 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from pytest) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages (from exceptiongroup>=1->pytest) (4.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62565eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RUNNING UNIT TEST FOR Q3\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Starting test_prepare_data\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Created test DataFrame with 4 rows\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - DataFrame after prepare_data has 2 rows\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Row 0: Duration=9.00, PULocationID=-1, DOLocationID=-1\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Row 1: Duration=8.00, PULocationID=1, DOLocationID=1\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Row count assertion passed\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Categorical column conversion assertion passed\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Duration calculation assertion passed\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - test_prepare_data completed successfully\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Starting test_path_functions\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Default input path: https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-03.parquet\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Using local path format for input\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Default output path: homework06/data/taxi_type=yellow_year=2023_month=03.parquet\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Using local path format for output\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Custom input path: test-input-2023-03.parquet\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - Custom output path: test-output-2023-03.parquet\n",
      "2025-07-04 17:23:12 - tests.test_batch_refactoring - INFO - test_path_functions completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform linux -- Python 3.9.23, pytest-8.4.1, pluggy-1.6.0 -- /opt/conda/envs/exp-tracking-env/bin/python\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices\n",
      "plugins: anyio-4.9.0\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "homework06/tests/test_batch_refactoring.py::test_prepare_data \u001b[32mPASSED\u001b[0m\n",
      "homework06/tests/test_batch_refactoring.py::test_path_functions \u001b[32mPASSED\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "==================================================\n",
      "ANSWER TO Q3: The expected dataframe should have 2 rows ✅\n",
      "==================================================\n",
      "S3_ENDPOINT_URL: http://localhost:4566\n",
      "OUTPUT_FILE_PATTERN: s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n",
      "INPUT_FILE_PATTERN: s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n",
      "2025-07-04 17:23:29 - __main__ - INFO - Setting up S3 bucket nyc-duration at http://localhost:4566\n",
      "2025-07-04 17:23:29 - __main__ - INFO - Checking if LocalStack is running at http://localhost:4566\n",
      "2025-07-04 17:23:29 - __main__ - INFO - Checking if bucket exists: nyc-duration\n",
      "2025-07-04 17:23:30 - __main__ - INFO - Bucket already exists: nyc-duration\n",
      "2025-07-04 17:23:30 - __main__ - INFO - Ensuring directory exists: s3://nyc-duration/in/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "2025-07-04 17:23:31 - __main__ - INFO - Ensuring directory exists: s3://nyc-duration/out/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "2025-07-04 17:23:32 - __main__ - INFO - S3 bucket setup complete: s3://nyc-duration/\n",
      "2025-07-04 17:23:32 - __main__ - INFO - \n",
      "Current bucket contents:\n",
      "2025-07-04 17:23:31          0 in/\n",
      "2025-07-04 17:23:32          0 out/\n",
      "2025-07-04 17:23:33 - __main__ - INFO - Setup completed successfully\n",
      "==================================================\n",
      "CHECKING S3 FOR Q4\n",
      "==================================================\n",
      "2025-07-04 17:10:59 nyc-duration\n",
      "\n",
      "==================================================\n",
      "ANSWER TO Q4: The correct option to use with AWS CLI for LocalStack is '--endpoint-url' ✅\n",
      "==================================================\n",
      "2025-07-04 17:25:18 - homework06.batch_refactoring - INFO - Starting prediction for year=2023, month=1\n",
      "2025-07-04 17:25:18 - homework06.batch_refactoring - INFO - Input file: s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-04 17:25:18 - homework06.batch_refactoring - INFO - Output file: s3://nyc-duration/out/2023-01.parquet\n",
      "2025-07-04 17:25:18 - homework06.batch_refactoring - INFO - Loading model...\n",
      "/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DictVectorizer from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-07-04 17:25:19 - homework06.batch_refactoring - INFO - Model loaded successfully from /workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/model.bin\n",
      "2025-07-04 17:25:19 - homework06.batch_refactoring - INFO - Reading data from s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-04 17:25:19 - homework06.batch_refactoring - INFO - Using S3 endpoint URL: http://localhost:4566\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/fsspec/registry.py\", line 257, in get_filesystem_class\n",
      "    register_implementation(protocol, _import_class(bit[\"class\"]))\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/fsspec/registry.py\", line 292, in _import_class\n",
      "    mod = importlib.import_module(mod)\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1030, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 984, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 's3fs'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/tests/test_integration.py\", line 57, in <module>\n",
      "    sum_predicted = test_batch_prediction()\n",
      "  File \"/workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/tests/test_integration.py\", line 36, in test_batch_prediction\n",
      "    main(year, month)\n",
      "  File \"/workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/batch_refactoring.py\", line 118, in main\n",
      "    df = read_data(input_file, categorical)\n",
      "  File \"/workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/batch_refactoring.py\", line 46, in read_data\n",
      "    df = pd.read_parquet(filename, storage_options=options)\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/pandas/io/parquet.py\", line 669, in read_parquet\n",
      "    return impl.read(\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/pandas/io/parquet.py\", line 258, in read\n",
      "    path_or_handle, handles, filesystem = _get_path_or_handle(\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/pandas/io/parquet.py\", line 123, in _get_path_or_handle\n",
      "    fs, path_or_handle = fsspec.core.url_to_fs(\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/fsspec/core.py\", line 403, in url_to_fs\n",
      "    chain = _un_chain(url, kwargs)\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/fsspec/core.py\", line 351, in _un_chain\n",
      "    cls = get_filesystem_class(protocol)\n",
      "  File \"/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/fsspec/registry.py\", line 259, in get_filesystem_class\n",
      "    raise ImportError(bit.get(\"err\")) from e\n",
      "ImportError: Install s3fs to access S3\n",
      "Error: \n",
      "Make sure to run the integration_test.py script first to create the test data.\n",
      "You can run it with: python homework06/tests/integration_test.py\n",
      "/workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices\n",
      "Error: \n",
      "Make sure to run the integration_test.py script first to create the test data.\n",
      "You can run it with: python homework06/tests/integration_test.py\n",
      "Attempting to save the file...\n",
      "An error occurred while saving the file: name 'df_input' is not defined\n",
      "2025-07-04 17:33:08          0 \n",
      "Checking file size for: s3://nyc-duration/in/2023-01.parquet\n",
      "\n",
      "Running command: aws --endpoint-url=http://localhost:4566 s3 ls s3://nyc-duration/in/2023-01.parquet\n",
      "\n",
      "Error executing AWS CLI command:\n",
      "\n",
      "\n",
      "Make sure LocalStack is running and the 'integration_test.py' script has been executed to create the test data.\n",
      "You can run it with: python homework06/tests/integration_test.py\n",
      "/workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices\n"
     ]
    }
   ],
   "source": [
    "# Run pytest from the notebook\n",
    "import pytest\n",
    "import sys\n",
    "from io import StringIO\n",
    "\n",
    "\n",
    "def run_test():\n",
    "    # Capture stdout to get test results\n",
    "    output = StringIO()\n",
    "    sys.stdout = output\n",
    "\n",
    "    # Run the test\n",
    "    pytest.main([\"-xvs\", \"homework06/tests/test_batch_refactoring.py\"])\n",
    "\n",
    "    # Restore stdout and return results\n",
    "    sys.stdout = sys.__stdout__\n",
    "    return output.getvalue()\n",
    "\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"RUNNING UNIT TEST FOR Q3\")\n",
    "print(\"=\" * 50)\n",
    "test_output = run_test()\n",
    "print(test_output)\n",
    "\n",
    "# Extract and display the answer explicitly\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"ANSWER TO Q3: The expected dataframe should have 2 rows ✅\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e19162",
   "metadata": {},
   "source": [
    "## Q4. Mocking S3 with Localstack \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55452691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_ENDPOINT_URL: http://localhost:4566\n",
      "OUTPUT_FILE_PATTERN: s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n",
      "INPUT_FILE_PATTERN: s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configure paths for local testing with LocalStack\n",
    "os.environ[\"INPUT_FILE_PATTERN\"] = \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "os.environ[\"OUTPUT_FILE_PATTERN\"] = (\n",
    "    \"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    ")\n",
    "os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "\n",
    "print(f\"S3_ENDPOINT_URL: {os.environ['S3_ENDPOINT_URL']}\")\n",
    "print(f\"OUTPUT_FILE_PATTERN: {os.environ['OUTPUT_FILE_PATTERN']}\")\n",
    "print(f\"INPUT_FILE_PATTERN: {os.environ['INPUT_FILE_PATTERN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d580a091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-04 17:57:39 - __main__ - INFO - Setting up S3 bucket nyc-duration at http://localhost:4566\n",
      "2025-07-04 17:57:39 - __main__ - INFO - Checking if LocalStack is running at http://localhost:4566\n",
      "2025-07-04 17:57:39 - __main__ - INFO - Checking if bucket exists: nyc-duration\n",
      "2025-07-04 17:57:40 - __main__ - INFO - Bucket already exists: nyc-duration\n",
      "2025-07-04 17:57:40 - __main__ - INFO - Ensuring directory exists: s3://nyc-duration/in/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "2025-07-04 17:57:41 - __main__ - INFO - Ensuring directory exists: s3://nyc-duration/out/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "2025-07-04 17:57:41 - __main__ - INFO - S3 bucket setup complete: s3://nyc-duration/\n",
      "2025-07-04 17:57:41 - __main__ - INFO - \n",
      "Current bucket contents:\n",
      "2025-07-04 17:57:40          0 in/\n",
      "2025-07-04 17:50:13       2958 in/2023-01.parquet\n",
      "2025-07-04 17:57:41          0 out/\n",
      "2025-07-04 17:57:42 - __main__ - INFO - Setup completed successfully\n"
     ]
    }
   ],
   "source": [
    "# Make sure the S3 bucket exists before running tests\n",
    "!python homework06/tests/setup_s3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3caf3b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3_ENDPOINT_URL: http://localhost:4566\n",
      "OUTPUT_FILE_PATTERN: s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n",
      "INPUT_FILE_PATTERN: s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configure paths for local testing with LocalStack\n",
    "os.environ[\"INPUT_FILE_PATTERN\"] = \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "os.environ[\"OUTPUT_FILE_PATTERN\"] = (\n",
    "    \"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    ")\n",
    "os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "\n",
    "print(f\"S3_ENDPOINT_URL: {os.environ['S3_ENDPOINT_URL']}\")\n",
    "print(f\"OUTPUT_FILE_PATTERN: {os.environ['OUTPUT_FILE_PATTERN']}\")\n",
    "print(f\"INPUT_FILE_PATTERN: {os.environ['INPUT_FILE_PATTERN']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1037738a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CHECKING S3 FOR Q4\n",
      "==================================================\n",
      "2025-07-04 17:10:59 nyc-duration\n",
      "\n",
      "==================================================\n",
      "ANSWER TO Q4: The correct option to use with AWS CLI for LocalStack is '--endpoint-url' ✅\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Verify the LocalStack bucket exists\n",
    "print(\"=\" * 50)\n",
    "print(\"CHECKING S3 FOR Q4\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "!aws --endpoint-url=http://localhost:4566 s3 ls\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\n",
    "    \"ANSWER TO Q4: The correct option to use with AWS CLI for LocalStack is '--endpoint-url' ✅\"\n",
    ")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f4d30",
   "metadata": {},
   "source": [
    "#The answer is we should use `--endpoint-url` in both the cases. answer below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319d95f",
   "metadata": {},
   "source": [
    "![alt text](<q4 bucket created.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b0155",
   "metadata": {},
   "source": [
    "![alt text](<q4 url.png>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c05dcf",
   "metadata": {},
   "source": [
    "## Q5. Creating test data\n",
    "\n",
    "We will pretend that this is data for January 2023.\n",
    "\n",
    "Run the `integration_test.py` script. After that, use AWS CLI to verify that the \n",
    "file was created. \n",
    "\n",
    "Use this snipped for saving the file:\n",
    "\n",
    "```python\n",
    "df_input.to_parquet(\n",
    "    input_file,\n",
    "    engine='pyarrow',\n",
    "    compression=None,\n",
    "    index=False,\n",
    "    storage_options=options\n",
    ")\n",
    "```\n",
    "\n",
    "What's the size of the file?\n",
    "\n",
    "* 3620 ✅\n",
    "* 23620\n",
    "* 43620\n",
    "* 63620"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5d994c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added /workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06 to Python path\n",
      "Added /workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices to Python path\n",
      "2025-07-07 06:46:26 - __main__ - INFO - Starting integration test\n",
      "2025-07-07 06:46:26 - __main__ - INFO - S3_ENDPOINT_URL: http://localhost:4566\n",
      "2025-07-07 06:46:26 - __main__ - INFO - INPUT_FILE_PATTERN: s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\n",
      "2025-07-07 06:46:26 - __main__ - INFO - OUTPUT_FILE_PATTERN: s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\n",
      "2025-07-07 06:46:26 - __main__ - INFO - Ensuring S3 bucket exists: nyc-duration\n",
      "2025-07-07 06:46:26 - __main__ - INFO - Running command: aws --endpoint-url=http://localhost:4566 s3 ls s3://nyc-duration || aws --endpoint-url=http://localhost:4566 s3 mb s3://nyc-duration\n",
      "                           PRE in/\n",
      "                           PRE out/\n",
      "2025-07-07 06:46:26 - __main__ - INFO - Creating directory: s3://nyc-duration/in/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "2025-07-07 06:46:27 - __main__ - INFO - Creating directory: s3://nyc-duration/out/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "2025-07-07 06:46:28 - __main__ - INFO - S3 bucket setup complete: s3://nyc-duration/\n",
      "2025-07-07 06:46:28 - __main__ - INFO - Creating test data\n",
      "2025-07-07 06:46:28 - __main__ - INFO - Created DataFrame with 4 rows\n",
      "2025-07-07 06:46:28 - __main__ - INFO - Saving test data to: s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-07 06:46:29 - aiobotocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-07-07 06:46:29 - __main__ - INFO - Test data saved successfully\n",
      "2025-07-07 06:46:29 - __main__ - INFO - Checking file size of s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-07 06:46:29 - __main__ - INFO - Running command: aws --endpoint-url=http://localhost:4566 s3 ls s3://nyc-duration/in/\n",
      "2025-07-07 06:46:27          0 \n",
      "2025-07-07 06:46:29       3241 2023-01.parquet\n",
      "2025-07-07 06:46:29 - __main__ - INFO - Running command: aws --endpoint-url=http://localhost:4566 s3api head-object --bucket nyc-duration --key in/2023-01.parquet\n",
      "2025-07-07 06:46:30 - __main__ - INFO - File size: 3241 bytes\n",
      "2025-07-07 06:46:30 - __main__ - INFO - File size: 3241 bytes\n",
      "2025-07-07 06:46:30 - __main__ - INFO - For Q5, the answer is closest to: 3620 bytes\n",
      "\n",
      "==================================================\n",
      "INTEGRATION TEST COMPLETED SUCCESSFULLY!\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python homework06/tests/integration_test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53231a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-07-07 06:46:27          0 \n",
      "2025-07-07 06:46:29       3241 2023-01.parquet\n"
     ]
    }
   ],
   "source": [
    "!aws --endpoint-url=http://localhost:4566 s3 ls s3://nyc-duration/in/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "189b27d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File size of test data: 3241 bytes\n",
      "\n",
      "Answer for Q5: The file size is closest to 3620 bytes ✅\n"
     ]
    }
   ],
   "source": [
    "# Check the file size of the test data file\n",
    "import os\n",
    "\n",
    "# Configure the S3 endpoint and paths\n",
    "os.environ[\"INPUT_FILE_PATTERN\"] = \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "# Define the path to the test data file\n",
    "year, month = 2023, 1\n",
    "input_file = os.environ[\"INPUT_FILE_PATTERN\"].format(year=year, month=month)\n",
    "\n",
    "# Use aws cli to get the file size\n",
    "import subprocess\n",
    "\n",
    "cmd = f\"aws --endpoint-url={os.environ['S3_ENDPOINT_URL']} s3 ls {input_file}\"\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    # Parse the output to get file size\n",
    "    output = result.stdout.strip()\n",
    "    if output:\n",
    "        parts = output.split()\n",
    "        if len(parts) >= 3:\n",
    "            size = parts[2]\n",
    "            print(f\"File size of test data: {size} bytes\")\n",
    "            print(\"\\nAnswer for Q5: The file size is closest to 3620 bytes ✅\")\n",
    "        else:\n",
    "            print(\"Could not parse file size from output:\", output)\n",
    "    else:\n",
    "        print(\"No output returned from aws command\")\n",
    "else:\n",
    "    print(f\"Error: {result.stderr}\")\n",
    "    print(\n",
    "        \"Make sure to run the integration_test.py script first to create the test data.\"\n",
    "    )\n",
    "    print(\"You can run it with: python homework06/tests/integration_test.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abfa76e",
   "metadata": {},
   "source": [
    "## Q6. Finish the integration test\n",
    "\n",
    "We can read from our localstack s3, but we also need to write to it.\n",
    "\n",
    "Let's run the `batch_refactoring.py` script for January 2023 (the fake data\n",
    "we created in Q5).\n",
    "\n",
    "We can do that from our integration test in Python: we can use\n",
    "`os.system` for doing that (there are other options too).\n",
    "\n",
    "Now it saves the result to localstack.\n",
    "\n",
    "The only thing we need to do now is to read this data and \n",
    "verify the result is correct. \n",
    "\n",
    "What's the sum of predicted durations for the test dataframe?\n",
    "\n",
    "* 13.08\n",
    "* 36.28 ✅\n",
    "* 69.28\n",
    "* 81.08"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13373cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "RUNNING INTEGRATION TEST FOR Q6\n",
      "==================================================\n",
      "Verifying S3 bucket exists...\n",
      "                           PRE in/\n",
      "                           PRE out/\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n",
      "{\n",
      "    \"ETag\": \"\\\"d41d8cd98f00b204e9800998ecf8427e\\\"\",\n",
      "    \"ChecksumCRC64NVME\": \"AAAAAAAAAAA=\",\n",
      "    \"ChecksumType\": \"FULL_OBJECT\",\n",
      "    \"ServerSideEncryption\": \"AES256\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 06:48:25 - homework06.batch_refactoring - INFO - Starting prediction for year=2023, month=1\n",
      "2025-07-07 06:48:25 - homework06.batch_refactoring - INFO - Input file: s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-07 06:48:25 - homework06.batch_refactoring - INFO - Output file: s3://nyc-duration/out/2023-01.parquet\n",
      "2025-07-07 06:48:25 - homework06.batch_refactoring - INFO - Loading model...\n",
      "/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DictVectorizer from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/exp-tracking-env/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator LinearRegression from version 1.5.0 when using version 1.6.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "2025-07-07 06:48:26 - homework06.batch_refactoring - INFO - Model loaded successfully from /workspaces/zoomcamp-mlops-mukesh-k-debata/06-best-practices/homework06/model.bin\n",
      "2025-07-07 06:48:26 - homework06.batch_refactoring - INFO - Reading data from s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-07 06:48:26 - homework06.batch_refactoring - INFO - Using S3 endpoint URL: http://localhost:4566\n",
      "2025-07-07 06:48:27 - aiobotocore.credentials - INFO - Found credentials in shared credentials file: ~/.aws/credentials\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Read 4 records from s3://nyc-duration/in/2023-01.parquet\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - After preparation: 2 records remaining\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Transforming features...\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Making predictions...\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - \n",
      "Predicted mean duration: 18.14\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - \n",
      "Predicted sum duration: 36.28\n",
      "\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Saving results to s3://nyc-duration/out/2023-01.parquet\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Using S3 endpoint URL for saving: http://localhost:4566\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Results saved successfully to s3://nyc-duration/out/2023-01.parquet\n",
      "2025-07-07 06:48:27 - homework06.batch_refactoring - INFO - Total predictions: 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch prediction completed. Output saved to: s3://nyc-duration/out/2023-01.parquet\n",
      "Successfully read output file: s3://nyc-duration/out/2023-01.parquet\n",
      "\n",
      "Output DataFrame:\n",
      "     ride_id  predicted_duration\n",
      "0  2023/01_0           23.197149\n",
      "1  2023/01_1           13.080101\n",
      "\n",
      "==================================================\n",
      "Sum of predicted durations: 36.28\n",
      "\n",
      "==================================================\n",
      "ANSWER TO Q6: The sum of predicted durations is 36.28\n",
      "The closest option is: 36.28 ✅\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def verify_s3_bucket_exists(bucket_name=\"nyc-duration\"):\n",
    "    \"\"\"Make sure the S3 bucket exists before running the test\"\"\"\n",
    "    s3_endpoint_url = os.getenv(\"S3_ENDPOINT_URL\", \"http://localhost:4566\")\n",
    "\n",
    "    # Create the bucket if it doesn't exist\n",
    "    cmd_check = f\"aws --endpoint-url={s3_endpoint_url} s3 ls s3://{bucket_name} || aws --endpoint-url={s3_endpoint_url} s3 mb s3://{bucket_name}\"\n",
    "    print(\"Verifying S3 bucket exists...\")\n",
    "    subprocess.run(cmd_check, shell=True, check=True)\n",
    "\n",
    "    # Create directories if needed\n",
    "    cmd_dirs = f\"aws --endpoint-url={s3_endpoint_url} s3api put-object --bucket {bucket_name} --key in/\"\n",
    "    subprocess.run(cmd_dirs, shell=True, check=True)\n",
    "    cmd_dirs = f\"aws --endpoint-url={s3_endpoint_url} s3api put-object --bucket {bucket_name} --key out/\"\n",
    "    subprocess.run(cmd_dirs, shell=True, check=True)\n",
    "\n",
    "    # Verify test data file exists\n",
    "    year, month = 2023, 1\n",
    "    input_pattern = os.getenv(\n",
    "        \"INPUT_FILE_PATTERN\", \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "    )\n",
    "    input_file = input_pattern.format(year=year, month=month)\n",
    "\n",
    "    cmd_check_file = f\"aws --endpoint-url={s3_endpoint_url} s3 ls {input_file}\"\n",
    "    result = subprocess.run(cmd_check_file, shell=True, capture_output=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"Test data file does not exist: {input_file}\")\n",
    "        print(\"Run the integration_test.py script first to create the test data.\")\n",
    "        print(\"Attempting to run it now...\")\n",
    "        subprocess.run([\"python\", \"homework06/tests/integration_test.py\"], check=True)\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def test_integration():\n",
    "    \"\"\"Integration test for batch prediction\n",
    "\n",
    "    1. First make sure the test data exists in S3\n",
    "    2. Run batch_refactoring.py for January 2023\n",
    "    3. Read the results and calculate sum of predictions\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"RUNNING INTEGRATION TEST FOR Q6\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Configure environment variables\n",
    "    os.environ[\"INPUT_FILE_PATTERN\"] = (\n",
    "        \"s3://nyc-duration/in/{year:04d}-{month:02d}.parquet\"\n",
    "    )\n",
    "    os.environ[\"OUTPUT_FILE_PATTERN\"] = (\n",
    "        \"s3://nyc-duration/out/{year:04d}-{month:02d}.parquet\"\n",
    "    )\n",
    "    os.environ[\"S3_ENDPOINT_URL\"] = \"http://localhost:4566\"\n",
    "\n",
    "    # Make sure the bucket and test data exist\n",
    "    verify_s3_bucket_exists()\n",
    "\n",
    "    # Set parameters for January 2023\n",
    "    year, month = 2023, 1\n",
    "    s3_endpoint_url = os.environ[\"S3_ENDPOINT_URL\"]\n",
    "\n",
    "    # S3 options for reading/writing with LocalStack\n",
    "    options = {\"client_kwargs\": {\"endpoint_url\": s3_endpoint_url}}\n",
    "\n",
    "    # Import and run the batch prediction script\n",
    "    from homework06.batch_refactoring import main\n",
    "\n",
    "    output_path = main(year, month)\n",
    "    print(f\"Batch prediction completed. Output saved to: {output_path}\")\n",
    "\n",
    "    # Define the output path (should be the same as returned by main)\n",
    "    output_pattern = os.environ[\"OUTPUT_FILE_PATTERN\"]\n",
    "    output_file = output_pattern.format(year=year, month=month)\n",
    "\n",
    "    # Read the results\n",
    "    try:\n",
    "        df_result = pd.read_parquet(output_file, storage_options=options)\n",
    "        print(f\"Successfully read output file: {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading output file: {e}\")\n",
    "        return\n",
    "\n",
    "    # Show the results\n",
    "    print(\"\\nOutput DataFrame:\")\n",
    "    print(df_result)\n",
    "\n",
    "    # Calculate sum of predicted durations\n",
    "    sum_pred = df_result[\"predicted_duration\"].sum()\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"Sum of predicted durations: {sum_pred:.2f}\")\n",
    "\n",
    "    # Check which option is closest to our result\n",
    "    options = [13.08, 36.28, 69.28, 81.08]\n",
    "    closest = min(options, key=lambda x: abs(x - sum_pred))\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"ANSWER TO Q6: The sum of predicted durations is {sum_pred:.2f}\")\n",
    "    print(f\"The closest option is: {closest} ✅\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "# Run the integration test\n",
    "test_integration()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exp-tracking-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
